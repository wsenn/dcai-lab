{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zwfWqPeA1zX0"
   },
   "source": [
    "# Lab â€” Label Errors\n",
    "\n",
    "This lab highlights data-centric AI techniques (using [confident learning](https://jair.org/index.php/jair/article/view/12125)) to improve the accuracy of an XGBoost classifier on a noisy dataset that has label errors.\n",
    "\n",
    "The DCAI techniques demonstrated in this lab involve optimizing the dataset itself rather than altering the model's architecture or hyperparameters. As a result, it is possible to achieve further improvements in accuracy by fine-tuning the model in conjunction with the newly enhanced data, but that is not the focus of this lab.\n",
    "\n",
    "In this lab, we will:\n",
    "\n",
    "- Establish a baseline [XGBoost](https://xgboost.readthedocs.io/) model accuracy on the original data\n",
    "- Automatically find mislabeled data points by:\n",
    "    - Computing out-of-sample predicted probabilities\n",
    "    - Estimating the number of label errors using confident learning\n",
    "    - Ranking errors, using the number of label errors as a cutoff in identifying issues\n",
    "- Remove the bad data\n",
    "- Retrain the exact same XGBoost model to see the improvement in test accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software installation\n",
    "\n",
    "This lab relies on a couple PyPI packages. If you don't have them installed, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.7 in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (1.7.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: pandas in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (1.3.5)\n",
      "Requirement already satisfied: cleanlab in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: scipy in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (from xgboost==1.7) (1.10.1)\n",
      "Requirement already satisfied: numpy in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (from xgboost==1.7) (1.21.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: termcolor>=2.0.0 in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (from cleanlab) (2.3.0)\n",
      "Requirement already satisfied: tqdm>=4.53.0 in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (from cleanlab) (4.65.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/williamsenn/miniforge3/envs/ai2023/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==1.7 scikit-learn pandas cleanlab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "je7P55z4RwX_"
   },
   "source": [
    "## Setup and Data Processing\n",
    "\n",
    "Let's take a look at the dataset used in this lab, a tabular dataset of student grades.\n",
    "\n",
    "The data includes three exam scores (numerical features), a written note (categorical feature with missing values), and a (noisy) letter grade (categorical label). Our aim is to train a model to classify the grade for each student based on the other features.\n",
    "\n",
    "In this dataset, 20% of the grade labels are actually incorrect (the `noisy_letter_grade` column). Synthetic noise was added to this dataset for the purpose of this lab. In this lab, we have access to the true letter grade each student should have received (the `letter_grade` column), which we use for evaluating both the underlying accuracy of model predictions and how well our approach detects which data are mislabeled. We are careful to only use these true grades for evaluation, not for model training.\n",
    "\n",
    "In the real world, you don't have access to the true labels (you only observe the `noisy_letter_grade`, not the true `letter_grade`). So when evaluating models in the real world, you have to be careful to make sure that your test set is free of error (using methods like those covered in this lab, ideally combined with human review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nQVmMBQOS43j",
    "outputId": "54a45659-ecb6-47fe-acfa-d0424027af47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f48f73</td>\n",
       "      <td>53</td>\n",
       "      <td>77</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0bd4e7</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>great participation +10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1795d</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cb9d7a</td>\n",
       "      <td>61</td>\n",
       "      <td>94</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9acca4</td>\n",
       "      <td>48</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>745c23</td>\n",
       "      <td>89</td>\n",
       "      <td>95</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24d6f2</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>97</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e5af86</td>\n",
       "      <td>71</td>\n",
       "      <td>82</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0aa465</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>96</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cac6eb</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>98</td>\n",
       "      <td>missed class frequently -10</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stud_ID  exam_1  exam_2  exam_3                        notes letter_grade  \\\n",
       "0  f48f73      53      77      93                          NaN            C   \n",
       "1  0bd4e7      81      64      80      great participation +10            B   \n",
       "2  e1795d      74      88      97                          NaN            B   \n",
       "3  cb9d7a      61      94      78                          NaN            C   \n",
       "4  9acca4      48      90      91                          NaN            C   \n",
       "5  745c23      89      95      72                          NaN            B   \n",
       "6  24d6f2       0      83      97   cheated on exam, gets 0pts            D   \n",
       "7  e5af86      71      82      97                          NaN            B   \n",
       "8  0aa465       0      56      96   cheated on exam, gets 0pts            F   \n",
       "9  cac6eb      75      80      98  missed class frequently -10            C   \n",
       "\n",
       "  noisy_letter_grade  \n",
       "0                  C  \n",
       "1                  B  \n",
       "2                  B  \n",
       "3                  C  \n",
       "4                  C  \n",
       "5                  B  \n",
       "6                  D  \n",
       "7                  B  \n",
       "8                  F  \n",
       "9                  C  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./student-grades.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f48f73</td>\n",
       "      <td>53</td>\n",
       "      <td>77</td>\n",
       "      <td>93</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0bd4e7</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1795d</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>97</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cb9d7a</td>\n",
       "      <td>61</td>\n",
       "      <td>94</td>\n",
       "      <td>78</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9acca4</td>\n",
       "      <td>48</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>745c23</td>\n",
       "      <td>89</td>\n",
       "      <td>95</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24d6f2</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e5af86</td>\n",
       "      <td>71</td>\n",
       "      <td>82</td>\n",
       "      <td>97</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0aa465</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cac6eb</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stud_ID  exam_1  exam_2  exam_3 notes  letter_grade  noisy_letter_grade\n",
       "0  f48f73      53      77      93     5             2                   2\n",
       "1  0bd4e7      81      64      80     2             1                   1\n",
       "2  e1795d      74      88      97     5             1                   1\n",
       "3  cb9d7a      61      94      78     5             2                   2\n",
       "4  9acca4      48      90      91     5             2                   2\n",
       "5  745c23      89      95      72     5             1                   1\n",
       "6  24d6f2       0      83      97     0             3                   3\n",
       "7  e5af86      71      82      97     5             1                   1\n",
       "8  0aa465       0      56      96     0             4                   4\n",
       "9  cac6eb      75      80      98     3             2                   2"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = df.copy()\n",
    "# Transform letter grades and notes to categorical numbers.\n",
    "# Necessary for XGBoost.\n",
    "df['letter_grade'] = preprocessing.LabelEncoder().fit_transform(df['letter_grade'])\n",
    "df['noisy_letter_grade'] = preprocessing.LabelEncoder().fit_transform(df['noisy_letter_grade'])\n",
    "df['notes'] = preprocessing.LabelEncoder().fit_transform(df[\"notes\"])\n",
    "df['notes'] = df['notes'].astype('category')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    292\n",
       "C    231\n",
       "A    194\n",
       "D    141\n",
       "F     83\n",
       "Name: letter_grade, dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c['letter_grade'].unique()\n",
    "df_c['letter_grade'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dVH_iciASD9F"
   },
   "source": [
    "# Get What We Need\n",
    "\n",
    "To apply confident learning (the technique explained in today's lecture), we need to obtain [**out-of-sample** predicted probabilities](https://docs.cleanlab.ai/stable/tutorials/pred_probs_cross_val.html#out-of-sample-predicted-probabilities) for all of our data. To do this, we can use K-fold cross validation: for each fold, we will train on some subset of our data and get predictions on the rest of the data that was _not_ used for training.\n",
    "\n",
    "We need to choose a model in order to do this. For this lab, we'll use [XGBoost](https://xgboost.readthedocs.io/), a library implementing gradient-boosted decision trees, a class of model commonly used for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCS19IqJsQUL",
    "outputId": "fd89b945-6793-4b3a-a017-7b19f8e6a29b"
   },
   "outputs": [],
   "source": [
    "# Prepare training data (remove labels from the dataframe) and labels\n",
    "data = df.drop(['stud_ID', 'letter_grade', 'noisy_letter_grade'], axis=1)\n",
    "labels = df['noisy_letter_grade']\n",
    "\n",
    "# XGBoost(experimental) supports categorical data.\n",
    "# Here we use default hyperparameters for simplicity.\n",
    "# Get out-of-sample predicted probabilities and check model accuracy.\n",
    "model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: getting out-of-sample predicted probabilities\n",
    "\n",
    "Compute out-of-sample predicted probabilities for every data point. You can do this manually using for loops and multiple invocations of model training and prediction, or you can use scikit-learn's [cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) (if you're using this function, take a look at the documentations, and in particular, the `method=` keyword argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_probs should be a Nx5 matrix of out-of-sample predicted probabilities, with N = len(data)\n",
    "\n",
    "pred_probs = cross_val_predict(estimator=model, X=data, y=labels, cv=5, method=\"predict_proba\") # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # out-of-sample pred probaiblity from scratch without cross_validation from sklearn?\n",
    "# cv = 5\n",
    "# N = len(data)\n",
    "# part_size = N // 5\n",
    "# remainder = N % 5\n",
    "# results = []\n",
    "# start = 0\n",
    "# for i in range(cv):\n",
    "#     end = start + part_size + (1 if i < remainder else 0)\n",
    "#     X_train = data.iloc[:start].append(data.iloc[end:])\n",
    "#     y_train = labels.iloc[:start].append(labels.iloc[end:])\n",
    "#     X_val = data.iloc[start:end]\n",
    "#     model.fit(X_train, y_train)\n",
    "#     model.predict_proba(X_val)\n",
    "#     results.append(model.predict_proba(data.iloc[start:end]))\n",
    "#     start = end\n",
    "# pred_probs = np.concatenate(results)\n",
    "# pred_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking model accuracy on original data\n",
    "\n",
    "Now that we have out-of-sample predicted probabilities, we can also check the model's (cross-val) accuracy on the original (noisy) data, so we'll have a baseline to compare our final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with original data: 67.4%\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(pred_probs, axis=1)\n",
    "acc_original = accuracy_score(preds, labels)\n",
    "print(f\"Accuracy with original data: {round(acc_original*100,1)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding label issues automatically\n",
    "\n",
    "We count label issues using confident learning. First, we need to compute class thresholds for the different classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: computing class thresholds\n",
    "\n",
    "Implement the Confident Learning algorithm for computing class thresholds for the 5 classes. You can refer to slide 26 from today's lecture or see equation 2 in [this paper](https://jair.org/index.php/jair/article/view/12125).\n",
    "\n",
    "The class threshold for each class is the model's expected (average) self-confidence for each class. In other words, to compute the threshold for a particular class, you can average the predicted probability for that class, for all datapoints that are labeled with that particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_class_thresholds(pred_probs: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "#     # YOUR CODE HERE\n",
    "#     class_mean_probs = []\n",
    "#     for label in np.unique(labels):\n",
    "#         label_indices = np.where(labels == label)[0]\n",
    "#         class_probs = pred_probs[label_indices, label]\n",
    "#         class_mean_probs.append(np.mean(class_probs))\n",
    "#     return np.array(class_mean_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_class_thresholds(pred_probs: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "#     # YOUR CODE HERE\n",
    "#     n_examples, n_classes = pred_probs.shape\n",
    "#     thresholds = np.zeros(n_classes)\n",
    "#     for k in range(n_classes):\n",
    "#         label_indices = np.where(labels == k)[0]\n",
    "#         thresholds[k] = np.mean(pred_probs[label_indices, k])\n",
    "#     return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_thresholds(pred_probs: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    n_examples, n_classes = pred_probs.shape\n",
    "    thresholds = np.zeros(n_classes)\n",
    "\n",
    "    # Create a boolean array indicating if an example belongs to a class\n",
    "    class_masks = np.equal(labels[:, None], np.arange(n_classes))\n",
    "\n",
    "    # Compute the mean prediction probability for each class\n",
    "    thresholds = np.sum(pred_probs * class_masks, axis=0) / np.sum(class_masks, axis=0)\n",
    "\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be a numpy array of length 5\n",
    "thresholds = compute_class_thresholds(pred_probs, labels.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72064697, 0.64154028, 0.67945671, 0.47879461, 0.45895547])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: constructing the confident joint\n",
    "\n",
    "Next, we compute the confident joint, a matrix that counts the number of label errors for each noisy label $\\tilde{y}$ and true label $y^*$. You can follow the algorithm that we walked through in slide 27 from today's lecture, or see equation 1 in [this paper](https://jair.org/index.php/jair/article/view/12125).\n",
    "\n",
    "The confident joint C is a K x K matrix (with K = 5 for this dataset), where `C[i][j]` is an estimate of the count of the number of data points with noisy label `i` and true label `j`. From lecture, recall that we put a data point in bin `(i, j)` if its given label is `i`, and its predicted probability for class `j` is above the threshold for class `j` (`thresholds[j]`). Each data point should only go in a single bin; if a data point's predicted probability is above the class threshold for multiple classes, it goes in the bin for which it has the highest predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72543688 0.64092258 0.65302584 0.47412649 0.44727632]\n",
      "[False False False False  True]\n"
     ]
    }
   ],
   "source": [
    "a = [0.1, 0.2, 0.3, 0.4, 0.5] > thresholds\n",
    "print(thresholds)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confident_joint_vectorized_v2(pred_probs: np.ndarray, labels: np.ndarray, thresholds: np.ndarray) -> np.ndarray:\n",
    "    n_examples, n_classes = pred_probs.shape\n",
    "\n",
    "    # Create a mask for the predicted probabilities that are greater than or equal to the thresholds\n",
    "    threshold_mask = pred_probs >= thresholds\n",
    "\n",
    "    # For each data point, find the index with the highest probability above the threshold\n",
    "    max_indices = np.argmax(pred_probs * threshold_mask, axis=1)\n",
    "\n",
    "    # Create a mask for the predictions that are above the thresholds\n",
    "    mask = (pred_probs[np.arange(n_examples), max_indices] >= thresholds[max_indices])\n",
    "\n",
    "    # Apply the mask to get the confident predictions\n",
    "    confident_labels = max_indices[mask]\n",
    "    ground_truth_labels = labels[mask]\n",
    "\n",
    "    # Compute the confusion matrix using numpy's add.at function\n",
    "    C = np.zeros((n_classes, n_classes), dtype=np.int)\n",
    "    np.add.at(C, (ground_truth_labels, confident_labels), 1)\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qs/dj4yss5527lb43nxdf_87g8r0000gn/T/ipykernel_16762/161146452.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  C = np.zeros((n_classes, n_classes), dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[157,   4,  18,   8,  14],\n",
       "       [  4, 145,   1,  42,  11],\n",
       "       [  2,   4, 103,   5,   8],\n",
       "       [  3,  37,   0,  76,  10],\n",
       "       [ 19,  16,  16,   8,  70]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = compute_confident_joint(pred_probs, labels.to_numpy(), thresholds)\n",
    "C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: count the number of label issues\n",
    "\n",
    "Now that we have the confident joint C, we can count the estimated number of label issues in our dataset. Recall that this is the sum of the off-diagonal entries (the cases where we estimate that a label has been flipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_label_issues = C.sum() - C.trace()\n",
    "num_label_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated noise rate: 24.4%\n"
     ]
    }
   ],
   "source": [
    "print('Estimated noise rate: {:.1f}%'.format(100*num_label_issues / pred_probs.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: filter out label issues\n",
    "\n",
    "In this lab, our approach to identifying issues is to rank the data points by a score (\"self-confidence\", the model's predicted probability for a data point's given label) and then take the top `num_label_issues` of those.\n",
    "\n",
    "First, we want to compute the model's _self-confidence_ for each data point. For a data point `i`, that is `pred_probs[i, labels[i]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9544596 , 0.8391949 , 0.9663064 , 0.68786603, 0.49425828],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should be a numpy array of length 941 of probabilities\n",
    "self_confidences = np.array([pred_probs[i, l]] for i, l in enumerate(labels)])\n",
    "self_confidences[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we rank the _indices_ of the data points by the self-confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be a numpy array of length 941 of integer indices\n",
    "ranked_indices = np.argsort(self_confidences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compute the indices of label issues as the top `num_label_issues` items in the `ranked_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_idx = ranked_indices[:num_label_issues]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of the highest-ranked data points (most likely to be label issues):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>b0306d</td>\n",
       "      <td>77</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>77c9c5</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>11d11f</td>\n",
       "      <td>98</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>3dd1e5</td>\n",
       "      <td>64</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>9cc5f8</td>\n",
       "      <td>59</td>\n",
       "      <td>37</td>\n",
       "      <td>75</td>\n",
       "      <td>missed homework frequently -10</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stud_ID  exam_1  exam_2  exam_3                           notes  \\\n",
       "689  b0306d      77      51      70                             NaN   \n",
       "637  77c9c5       0      79      65      cheated on exam, gets 0pts   \n",
       "36   11d11f      98      69      90                             NaN   \n",
       "709  3dd1e5      64      70      86                             NaN   \n",
       "937  9cc5f8      59      37      75  missed homework frequently -10   \n",
       "\n",
       "    letter_grade noisy_letter_grade  \n",
       "689            D                  B  \n",
       "637            F                  A  \n",
       "36             B                  D  \n",
       "709            C                  F  \n",
       "937            F                  F  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_c.iloc[ranked_indices[:5]]\n",
    "df_c.iloc[ranked_indices[:5]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PrvJHkPzSq6Q"
   },
   "source": [
    "# How'd We Do?\n",
    "\n",
    "Let's go a step further and see how we did at automatically identifying which data points are mislabeled. If we take the intersection of the labels errors identified by Confident Learning and the true label errors, we see that our approach was able to identify 75% of the label errors correctly (based on predictions from a model that is only 67% accurate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9O2a6urWc1DA",
    "outputId": "f88b5ce6-33f2-4ef6-e774-19013c33f0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of errors found: 0.5%\n"
     ]
    }
   ],
   "source": [
    "# Computing percentage of true errors identified. \n",
    "true_error_idx = df[df.letter_grade != df.noisy_letter_grade].index.values\n",
    "cl_acc = len(set(true_error_idx).intersection(set(issue_idx)))/len(true_error_idx)\n",
    "print(f\"Percentage of errors found: {round(cl_acc*100,1)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YzxXoDOqSzn-"
   },
   "source": [
    "# Train a More Robust Model\n",
    "\n",
    "Now that we have the indices of potential label errors within our data, let's remove them from our data, retrain our model, and see what improvement we can gain.\n",
    "\n",
    "Keep in mind that our baseline model from above, trained on the original data using the `noisy_letter_grade` as the prediction label, achieved a cross-validation accuracy of 67%.\n",
    "\n",
    "Let's use a very simple method to handle these label errors and just drop them entirely from the data and retrain our exact same `XGBClassifier`. In a real-world application, a better approach might be to have humans review the issues and _correct_ the labels rather than dropping the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsQFmy7xgSUa",
    "outputId": "a33e17ab-c197-4f95-c9c1-0473c16af313"
   },
   "outputs": [],
   "source": [
    "# Remove the label errors found by Confident Learning\n",
    "data = df.drop(issue_idx)\n",
    "clean_labels = data['noisy_letter_grade']\n",
    "data = data.drop(['stud_ID', 'letter_grade', 'noisy_letter_grade'], axis=1)\n",
    "\n",
    "# Train a more robust classifier with less erroneous data\n",
    "model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "clean_pred_probs = cross_val_predict(model, data, clean_labels, method='predict_proba')\n",
    "clean_preds = np.argmax(clean_pred_probs, axis=1)\n",
    "\n",
    "acc_clean = accuracy_score(clean_preds, clean_labels)\n",
    "print(f\"Accuracy with original data: {round(acc_original*100, 1)}%\")\n",
    "print(f\"Accuracy with errors found by Confident Learning removed: {round(acc_clean*100, 1)}%\")\n",
    "\n",
    "# Compute reduction in error.\n",
    "err = ((1-acc_original)-(1-acc_clean))/(1-acc_original)\n",
    "print(f\"Reduction in error: {round(err*100,1)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9J9clVf1UzQZ"
   },
   "source": [
    "After removing the suspected label issues, our model's new cross-validation accuracy is now 90%, which means we **reduced the error-rate of the model by 70%** (the original model had 67% accuracy). \n",
    "\n",
    "**Note: throughout this entire process we never changed any code related to model architecture/hyperparameters, training, or data preprocessing!  This improvement is strictly coming from increasing the quality of our data which leaves additional room for additional optimizations on the modeling side.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-W-Lo82SVp7I"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "For the student grades dataset, we found that simply dropping identified label errors and retraining the model resulted in a 70% reduction in prediction error on our classification problem (with accuracy improving from 67% to 90%).\n",
    "\n",
    "An implementation of the Confident Learning algorithm (and much more) is available in the [cleanlab](https://github.com/cleanlab/cleanlab) library on GitHub. This is how today's lab assignment can be done in a single line of code with Cleanlab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "\n",
    "cl_issue_idx = cleanlab.filter.find_label_issues(labels, pred_probs, return_indices_ranked_by='self_confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.iloc[cl_issue_idx[:5]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Advanced topic_: you might notice that the above `cl_issue_idx` differs in length (by a little bit) from our `issue_idx`. The reason for this is that we implemented a slightly simplified version of the algorithm in this lab. We skipped a calibration step after computing the confident joint that makes the confident joint have the true noisy prior $p(labels)$ (summed over columns for each row) and also add up to the total number of examples. If you're interested in the details of this, see equation 3 and the subsequent explanation in the [paper](https://jair.org/index.php/jair/article/view/12125)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
